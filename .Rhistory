## EM iterations
while ((num_iter < max_iter) & !converged) {
## E-step
KFS <- kalmanCpp(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde)
#KFS <- kalman(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde)
at_n = KFS$at_n             # state mean: k x (n+1) matrix (t=0,...,n)
Pt_n = KFS$Pt_n             # state covariance: k x k x (n+1) array (t=0,...,n)
Pt_tlag_n = KFS$Pt_tlag_n   # state covariance with lag: k x k x n array (t=1,...,n)
loglik = KFS$logl           # log-likelihood value
num_iter = num_iter + 1     # new iteration
## M-step
## Initial state mean and covariance update
a0_0 = as.matrix(at_n[,1])
P0_0 = as.matrix(Pt_n[,,1])
## State transition equation parameter updates: A.tilde and Sigma.u.tilde
## required sums for the update equations
E_Ft_Ftlag = at_n[1:r,2:(n+1)] %*% t(at_n[1:r,1:n]) + apply(Pt_tlag_n[1:r,1:r,],c(1,2),sum)
E_Ftlag_Ftlag = at_n[1:r,1:n] %*% t(at_n[1:r,1:n]) + apply(Pt_n[1:r,1:r,1:n],c(1,2),sum)
E_Ft_Ft = at_n[1:r,2:(n+1)] %*% t(at_n[1:r,2:(n+1)]) + apply(Pt_n[1:r,1:r,2:(n+1)],c(1,2),sum)
E_Ftlag_Ft = at_n[1:r,1:n] %*% t(at_n[1:r,2:(n+1)]) + apply(Pt_tlag_n[1:r,1:r,],c(1,2),sum)
## A update
A = E_Ft_Ftlag %*% solve(E_Ftlag_Ftlag)
A.tilde = A
## Sig_u update
Sig_u = (E_Ft_Ft - A %*% E_Ftlag_Ft)/n
Sig.u.tilde = Sig_u
## Measurement equation parameter updates: Lambda.tilde and Sigma.eta
## Lambda.tilde update (ADMM algorithm for lasso regularisation - see paper)
y = t(X)
y[is.na(y)] = 0
## Calculate components A, B and C in the fast algorithm for Lambda computation (see paper)
##
## At: r x r x n array
## Bt: p x p matrix
## Ct: p x r matrix
At = array(NA, dim=c(r, r, n))
Bt = matrix(NA, nrow = p, ncol = n)
Ct = 0
Sigma.eta.inv = 1/diag(Sigma.eta)
for(t in 1:n){
At[,,t] = at_n[1:r,t+1] %*% t(at_n[1:r,t+1]) + Pt_n[1:r,1:r,t+1]
Bt[,t] = W[t,]*Sigma.eta.inv*W[t,]
Ct = Ct + as.matrix(Bt[,t]*y[,t]) %*% at_n[1:r,t+1]
}
## Estimating loadings: with or without sparsity
if(sparse){
## Solve fast algorithm for Lambda
D_cube = solveCube(At, Bt, nu = 1)
sol = solveLambda(D_cube, Ct, nu = 1, alpha = alpha.lasso)
Lambda = sol$Z
## Lambda.tilde construction
Lambda.tilde = Lambda
}else{
D_cube = solveCube(At, Bt, nu = 0)
Lambda = fastLambda(D_cube, Ct)
Lambda.tilde = Lambda
}
## Sigma.eta update
Sig_e_new = 0
I = rep(1,p)
for(t in 1:n){
Sig_e_new = Sig_e_new + (tcrossprod(W[t,]*y[,t]) - tcrossprod(W[t,]*y[,t],W[t,]*Lambda%*%at_n[1:r,t+1])
- tcrossprod(W[t,]*Lambda%*%at_n[1:r,t+1],W[t,]*y[,t]) + tcrossprod(W[t,]*Lambda%*%(At[,,t]),W[t,]*Lambda)
+ diag((I - W[t,])*diag(Sigma.eta)*(I - W[t,])))
}
Sig_e = Sig_e_new/n
Sig_e = diag(diag(Sig_e))
Sigma.eta = Sig_e
if(any(diag(Sigma.eta)<0.001)){
converged = TRUE
previous_loglik <- loglik
loglik.store[num_iter] = loglik
}else{
## Check convergence
converged <- emConverged(loglik, previous_loglik, threshold)
previous_loglik <- loglik
loglik.store[num_iter] = loglik
}
}
output <- list('a0_0' = a0_0, 'P0_0' = P0_0, 'A.tilde' = A.tilde, 'Sigma.u.tilde' = Sigma.u.tilde,
'Lambda.tilde' = Lambda.tilde, 'Sigma.eta' = Sigma.eta, 'loglik.store' = loglik.store,
'converged' = converged, 'num_iter' = num_iter)
return(output)
}
}
## 1. PVI
indicators = cbind(new_hicp,
new_fred,
new_opec,
new_google)
target = new_pvi # SA
#target = new_ppi # NSA
#target = tour
# remove first row (Jan 2004) as this is missing a lot as we first differenced
# data is from Feb 2004 to Nov 2022
target = target[-1,]
indicators = indicators[-1,]
# test
X = cbind(target, indicators)
fit.dfm <- Sparse_DFM(X, r = 7, alg = 'EM', err = 'IID', standardize = TRUE)
## 2. PPI
indicators = cbind(new_hicp,
new_fred,
new_opec,
new_google)
#target = new_pvi # SA
target = new_ppi # NSA
#target = tour
# remove first row (Jan 2004) as this is missing a lot as we first differenced
# data is from Feb 2004 to Nov 2022
target = target[-1,]
indicators = indicators[-1,]
# test
X = cbind(target, indicators)
fit.dfm2 <- Sparse_DFM(X, r = 7, alg = 'EM', err = 'IID', standardize = TRUE)
## 3. tour
indicators = cbind(new_hicp,
new_fred,
new_opec,
new_google)
# target = new_pvi # SA
#target = new_ppi # NSA
target = tour
# remove first row (Jan 2004) as this is missing a lot as we first differenced
# data is from Feb 2004 to Nov 2022
target = target[-1,]
indicators = indicators[-1,]
# test
X = cbind(target, indicators)
fit.dfm3 <- Sparse_DFM(X, r = 7, alg = 'EM', err = 'IID', standardize = TRUE)
indicators = cbind(new_hicp,
new_fred,
new_opec,
new_google)
target = new_pvi # SA
target = target[-1,]
indicators = indicators[-1,]
X = cbind(target, indicators)
r = 7
alg = 'EM'
err = 'IID'
standardize = TRUE
max_iter=500
threshold=1e-4
n = dim(X)[1]
p = dim(X)[2]
k = r + p
X.raw = X
X.scale = scale(X)
X.mean = attr(X.scale, "scaled:center")
X.sd = attr(X.scale, "scaled:scale")
if(standardize){
X = X.scale
}
sparse = FALSE
initialise <- initPCA(X,r,err)
a0_0 = initialise$a0_0
P0_0 = initialise$P0_0
A.tilde = initialise$A.tilde
Lambda.tilde = initialise$Lambda.tilde
Sigma.u.tilde = initialise$Sigma.u.tilde
Sigma.eta = initialise$Sigma.eta
factors.PCA = initialise$factors.pca
loadings.PCA = initialise$loadings.pca
EM <- function(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde, alpha.lasso = 0.1, err = 'AR1', sparse = TRUE, max_iter=500, threshold=1e-4) {
##  Sparsified EM algorithm for IID or AR(1) idio errors (adapted from Banbura and Modugno (2014))
##
##  E-step: Kalman filter and smoother using parameters estimated in M-step to obtain
##          state mean, covariance and lagged-covariance. Log-likelihood calculated here.
##
##  M-step: Maximisation of expected log-likelihood equations using E-step output.
##          With LASSO regularisation applied to the loadings matrix.
##
##  Convergence: Uses the log-likelihood convergence rule from from Doz (2012)
##  Input:
##
##  X: n x p, matrix of (stationary) time series
##  a0_0: 1 x k, initial state mean vector
##  P0_0: k x k, initial state covariance matrix
##  A.tilde: k x k, initial state transition matrix
##  Lambda.tilde: p x k, initial measurement matrix
##  Sigma.eta: p x p, initial measurement equation residuals covariance matrix (diagonal)
##  Sigma.u.tilde: k x k, initial state equation residuals covariance matrix
##  alpha.lasso: lasso tuning parameter value (>= 0). Default set to 0.1.
##  err: idiosyncratic error structure, 'AR1' or 'IID'. Default is 'AR1'.
##  sparse: if TRUE, lasso regularisation applied to the loadings. Default is TRUE.
##  max_iter: maximum number of iterations for the EM algorithm. Default is 500.
##  threshold: threshold for log-likelihood convergence check. Default is 1e-4.
##
##  NOTE: For the DFM with AR(1) errors we have that k = r + p where r is
##        the number of factors and p is the number of variables.
##        Otherwise, k = r.
library(pracma)
if(err == 'AR1'){
## Initialise
n = dim(X)[1]
p = dim(X)[2]
k = dim(A.tilde)[1]
r = k-p
previous_loglik = -.Machine$double.xmax   # at least 2 iterations
num_iter = 0        # counter begins at 0
converged = 0       # convergence initialised to FALSE
loglik.store = c()  # store log likelihoods
W = 1*!is.na(X)   # data availability - n x p - 0 for missing, 1 for observed
## EM iterations
while ((num_iter < max_iter) & !converged) {
## E-step
KFS <- kalmanCpp(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde)
at_n = KFS$at_n             # state mean: k x (n+1) matrix (t=0,...,n)
Pt_n = KFS$Pt_n             # state covariance: k x k x (n+1) array (t=0,...,n)
Pt_tlag_n = KFS$Pt_tlag_n   # state covariance with lag: k x k x n array (t=1,...,n)
loglik = KFS$logl           # log-likelihood value
num_iter = num_iter + 1     # new iteration
## M-step
## Initial state mean and covariance update
a0_0 = as.matrix(at_n[,1])
P_F = as.matrix(Pt_n[1:r,1:r,1])
P_E = diag(diag(as.matrix(Pt_n[(r+1):k,(r+1):k,1])))
P0_0 = blkdiag(P_F, P_E)
## State transition equation parameter updates: A.tilde and Sigma.u.tilde
## required sums for the update equations
E_Ft_Ftlag = at_n[1:r,2:(n+1)] %*% t(at_n[1:r,1:n]) + apply(Pt_tlag_n[1:r,1:r,],c(1,2),sum)
E_Ftlag_Ftlag = at_n[1:r,1:n] %*% t(at_n[1:r,1:n]) + apply(Pt_n[1:r,1:r,1:n],c(1,2),sum)
E_Ft_Ft = at_n[1:r,2:(n+1)] %*% t(at_n[1:r,2:(n+1)]) + apply(Pt_n[1:r,1:r,2:(n+1)],c(1,2),sum)
E_Ftlag_Ft = at_n[1:r,1:n] %*% t(at_n[1:r,2:(n+1)]) + apply(Pt_tlag_n[1:r,1:r,],c(1,2),sum)
E_et_etlag = diag(diag(at_n[(r+1):k,2:(n+1)] %*% t(at_n[(r+1):k,1:n])) + diag(apply(Pt_tlag_n[(r+1):k,(r+1):k,],c(1,2),sum)))
E_etlag_etlag = diag(diag(at_n[(r+1):k,1:n] %*% t(at_n[(r+1):k,1:n])) + diag(apply(Pt_n[(r+1):k,(r+1):k,1:n],c(1,2),sum)))
E_et_et = diag(diag(at_n[(r+1):k,2:(n+1)] %*% t(at_n[(r+1):k,2:(n+1)])) + diag(apply(Pt_n[(r+1):k,(r+1):k,2:(n+1)],c(1,2),sum)))
E_etlag_et = diag(diag(at_n[(r+1):k,1:n] %*% t(at_n[(r+1):k,2:(n+1)])) + diag(apply(Pt_tlag_n[(r+1):k,(r+1):k,],c(1,2),sum)))
## A update
A = E_Ft_Ftlag %*% solve(E_Ftlag_Ftlag)
## Sig_u update
Sig_u = (E_Ft_Ft - A %*% E_Ftlag_Ft)/n
## Phi update
Phi = E_et_etlag %*% diag(1 / diag(E_etlag_etlag))
## Sig_epsilon update
Sig_epsilon = (E_et_et - Phi %*% t(E_etlag_et)) / n
## A.tilde construction
A.tilde = blkdiag(A, Phi)
## Sigma.u.tilde construction
Sigma.u.tilde = blkdiag(Sig_u, Sig_epsilon)
## Measurement equation parameter updates: Lambda.tilde and Sigma.eta
## Sigma.eta update (just the same)
kappa = 1e-4
Sigma.eta = kappa*diag(p)
## Lambda.tilde update (ADMM algorithm for lasso regularisation - see paper)
y = t(X)
y[is.na(y)] = 0
## Calculate components A, B and C in the fast algorithm for Lambda computation (see paper)
##
## At: r x r x n array
## Bt: p x p matrix
## Ct: p x r matrix
At = array(NA, dim=c(r, r, n))
Bt = t(W)
Ct = 0
for(t in 1:n){
At[,,t] = at_n[1:r,t+1] %*% t(at_n[1:r,t+1]) + Pt_n[1:r,1:r,t+1]
Ct = Ct + (as.matrix(Bt[,t]*y[,t]) %*% at_n[1:r,t+1]) - (diag(Bt[,t]) %*% (at_n[(r+1):k,t+1] %*% t(at_n[1:r,t+1]) + Pt_n[(r+1):k,1:r,t+1]))
}
## Estimating loadings: with or without sparsity
if(sparse){
## Solve fast algorithm for Lambda
D_cube = solveCube(At, Bt, nu = kappa)
sol = solveLambda(D_cube, Ct, nu = kappa, alpha = alpha.lasso)
Lambda = sol$Z
## Lambda.tilde construction
Lambda.tilde = cbind(Lambda, diag(p))
}else{
D_cube = solveCube(At, Bt, nu = 0)
Lambda = fastLambda(D_cube, Ct)
Lambda.tilde = cbind(Lambda, diag(p))
}
## Check convergence
converged <- emConverged(loglik, previous_loglik, threshold)
previous_loglik <- loglik
loglik.store[num_iter] = loglik
}
output <- list('a0_0' = a0_0, 'P0_0' = P0_0, 'A.tilde' = A.tilde, 'Sigma.u.tilde' = Sigma.u.tilde,
'Lambda.tilde' = Lambda.tilde, 'Sigma.eta' = Sigma.eta, 'loglik.store' = loglik.store,
'converged' = converged, 'num_iter' = num_iter)
return(output)
}else {
## Initialise
n = dim(X)[1]
p = dim(X)[2]
k = dim(A.tilde)[1]
r = k
previous_loglik = -.Machine$double.xmax   # at least 2 iterations
num_iter = 0        # counter begins at 0
converged = 0       # convergence initialised to FALSE
loglik.store = c()  # store log likelihoods
W = 1*!is.na(X)   # data availability - n x p - 0 for missing, 1 for observed
## EM iterations
while ((num_iter < max_iter) & !converged) {
## E-step
KFS <- kalmanCpp(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde)
#KFS <- kalman(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde)
at_n = KFS$at_n             # state mean: k x (n+1) matrix (t=0,...,n)
Pt_n = KFS$Pt_n             # state covariance: k x k x (n+1) array (t=0,...,n)
Pt_tlag_n = KFS$Pt_tlag_n   # state covariance with lag: k x k x n array (t=1,...,n)
loglik = KFS$logl           # log-likelihood value
print(num_iter)
print(loglik)
num_iter = num_iter + 1     # new iteration
## M-step
## Initial state mean and covariance update
a0_0 = as.matrix(at_n[,1])
P0_0 = as.matrix(Pt_n[,,1])
## State transition equation parameter updates: A.tilde and Sigma.u.tilde
## required sums for the update equations
E_Ft_Ftlag = at_n[1:r,2:(n+1)] %*% t(at_n[1:r,1:n]) + apply(Pt_tlag_n[1:r,1:r,],c(1,2),sum)
E_Ftlag_Ftlag = at_n[1:r,1:n] %*% t(at_n[1:r,1:n]) + apply(Pt_n[1:r,1:r,1:n],c(1,2),sum)
E_Ft_Ft = at_n[1:r,2:(n+1)] %*% t(at_n[1:r,2:(n+1)]) + apply(Pt_n[1:r,1:r,2:(n+1)],c(1,2),sum)
E_Ftlag_Ft = at_n[1:r,1:n] %*% t(at_n[1:r,2:(n+1)]) + apply(Pt_tlag_n[1:r,1:r,],c(1,2),sum)
## A update
A = E_Ft_Ftlag %*% solve(E_Ftlag_Ftlag)
A.tilde = A
## Sig_u update
Sig_u = (E_Ft_Ft - A %*% E_Ftlag_Ft)/n
Sig.u.tilde = Sig_u
## Measurement equation parameter updates: Lambda.tilde and Sigma.eta
## Lambda.tilde update (ADMM algorithm for lasso regularisation - see paper)
y = t(X)
y[is.na(y)] = 0
## Calculate components A, B and C in the fast algorithm for Lambda computation (see paper)
##
## At: r x r x n array
## Bt: p x p matrix
## Ct: p x r matrix
At = array(NA, dim=c(r, r, n))
Bt = matrix(NA, nrow = p, ncol = n)
Ct = 0
Sigma.eta.inv = 1/diag(Sigma.eta)
for(t in 1:n){
At[,,t] = at_n[1:r,t+1] %*% t(at_n[1:r,t+1]) + Pt_n[1:r,1:r,t+1]
Bt[,t] = W[t,]*Sigma.eta.inv*W[t,]
Ct = Ct + as.matrix(Bt[,t]*y[,t]) %*% at_n[1:r,t+1]
}
## Estimating loadings: with or without sparsity
if(sparse){
## Solve fast algorithm for Lambda
D_cube = solveCube(At, Bt, nu = 1)
sol = solveLambda(D_cube, Ct, nu = 1, alpha = alpha.lasso)
Lambda = sol$Z
## Lambda.tilde construction
Lambda.tilde = Lambda
}else{
D_cube = solveCube(At, Bt, nu = 0)
Lambda = fastLambda(D_cube, Ct)
Lambda.tilde = Lambda
}
## Sigma.eta update
Sig_e_new = 0
I = rep(1,p)
for(t in 1:n){
Sig_e_new = Sig_e_new + (tcrossprod(W[t,]*y[,t]) - tcrossprod(W[t,]*y[,t],W[t,]*Lambda%*%at_n[1:r,t+1])
- tcrossprod(W[t,]*Lambda%*%at_n[1:r,t+1],W[t,]*y[,t]) + tcrossprod(W[t,]*Lambda%*%(At[,,t]),W[t,]*Lambda)
+ diag((I - W[t,])*diag(Sigma.eta)*(I - W[t,])))
}
Sig_e = Sig_e_new/n
Sig_e = diag(diag(Sig_e))
Sigma.eta = Sig_e
if(any(diag(Sigma.eta)<0.001)){
converged = TRUE
previous_loglik <- loglik
loglik.store[num_iter] = loglik
}else{
## Check convergence
converged <- emConverged(loglik, previous_loglik, threshold)
previous_loglik <- loglik
loglik.store[num_iter] = loglik
}
}
output <- list('a0_0' = a0_0, 'P0_0' = P0_0, 'A.tilde' = A.tilde, 'Sigma.u.tilde' = Sigma.u.tilde,
'Lambda.tilde' = Lambda.tilde, 'Sigma.eta' = Sigma.eta, 'loglik.store' = loglik.store,
'converged' = converged, 'num_iter' = num_iter)
return(output)
}
}
# EM iterations function
EM.fit <- EM(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde,
err = err, sparse = FALSE, max_iter = max_iter, threshold = threshold)
n = dim(X)[1]
p = dim(X)[2]
k = dim(A.tilde)[1]
r = k
previous_loglik = -.Machine$double.xmax   # at least 2 iterations
num_iter = 0        # counter begins at 0
converged = 0       # convergence initialised to FALSE
loglik.store = c()  # store log likelihoods
W = 1*!is.na(X)   # data availability - n x p - 0 for missing, 1 for observed
KFS <- kalmanCpp(X, a0_0, P0_0, A.tilde, Lambda.tilde, Sigma.eta, Sigma.u.tilde)
at_n = KFS$at_n             # state mean: k x (n+1) matrix (t=0,...,n)
Pt_n = KFS$Pt_n             # state covariance: k x k x (n+1) array (t=0,...,n)
Pt_tlag_n = KFS$Pt_tlag_n   # state covariance with lag: k x k x n array (t=1,...,n)
loglik = KFS$logl           # log-likelihood value
load("C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Eurostat Comp/January/jandata.RData")
load('/home/mosleyl/tsclient/mosleyl/OneDrive - Lancaster University/PhD/Eurostat Comp/January/jandata.RData')
fred_data_nsa = fred_data_nsa[,-1] # error if do not remove first column
nm = nrow(fred_data_nsa) # 396 rows: Jan 1990 - Dec 2022
# excluding series with NA for Dec
fff = which(colSums(is.na(fred_data_nsa[nm,]))==0)
ff2 = fred_data_nsa[,fff]
# exclude series with more than 150 NAs
fff = which(colSums(is.na(ff2))<150)
fred = ff2[,fff]
rm(nm)
rm(fff)
rm(ff2)
rm(fred_data_nsa)
# make FRED time series
fred = ts(fred, start = c(1990,1), frequency = 12)
# NEW: window from 2004
fred = window(fred, start = c(2004,1), end = end(fred))
## Sort out Opec data - up to Dec
opec = ts(opec_data[,2], start = c(2003,1), frequency = 12)
opec2 = ts(NA, start = c(1990,1), end = c(2002,12), frequency = 12)
opec = ts(as.numeric(c(opec2, opec)), start = c(1990,1), frequency = 12)
# NEW: window from 2004
opec = window(opec, start = c(2004,1), end = end(opec))
## Sort out google trends data - available to Dec
EUc <- c("AT","EE","BE","BG","CY","CZ","DK","DE","IE","ES","FR","HR","LV","LT","LU","HU","NL","PL","PT","RO","SI","SK","FI","SE")
words <- c("cruise",
"hotel",
"camping",
"b&b",
"travel",
"booking",
"restaurant",
"energy",
"fly",
"gas",
"petroleum",
"microchip")
# make separate data frames for each country and combine
google_data = c()
for(i in 1:length(EUc)){
gt.country = as.data.frame(gtrend_data[,EUc[i]])
gt.country.mat = matrix(gt.country[[1]], ncol = 12)
colnames(gt.country.mat) = words
# assign(paste0("GT.", EUc[i]), gt.country.mat)
google_data = cbind(google_data, gt.country.mat)
}
# remove any column that is all 0
google_data = google_data[,-which(colSums(google_data)==0)]
google = ts(google_data, start = c(2004,1), frequency = 12)
# NEW: remove this stuff before 2004
# google2 = ts(matrix(NA, nrow=168, ncol=ncol(google_data)), start = c(1990, 1), end = c(2003,12), frequency = 12)
# google = ts(rbind(google2, google), start = c(1990,1), frequency = 12)
## Sort out eurostat hicp data - available to Nov
hicp = ts(ei_cphi_m[,-1], start = c(1996,1), frequency = 12)
hicp = window(hicp, start = c(2004,1), end = end(hicp))
hicp.nov = rep(NA, ncol(hicp))
hicp = ts(rbind(hicp, hicp.nov), start = c(2004,1), frequency = 12)
# NEW: remove this and start from 2004
# hicp2 = ts(matrix(NA, nrow = 72, ncol = ncol(hicp)), start = c(1990,1), end = c(1995,12), frequency = 12)
# hicp = ts(rbind(hicp2, hicp), start = c(1990,1), frequency = 12)
# colnames(hicp) = colnames(ei_cphi_m[,-1])
#
#
# # add NA in Nov row
# hicp.nov = rep(NA, ncol(hicp))
# hicp = ts(rbind(hicp, hicp.nov), start = c(1990,1), frequency = 12)
## Target 1: Production volume in industry (PVI) - up to Sept - NA for Oct and Nov
pvi = ts(sts_inpr_m[,-1], start = c(1990,1), frequency = 12)
pvi.na = matrix(NA, nrow = 2, ncol = ncol(pvi))
pvi = ts(rbind(pvi, pvi.na), start = c(1990,1), frequency = 12)
# NEW: from 2004
pvi = window(pvi, start = c(2004,1), end = end(pvi))
pvi_jan2004 = pvi[1,]
## Target 2: Producer prices in industry (PPI) - up to Oct - NA for Nov
ppi = ts(sts_inppd_m[,-1], start = c(1990,1), frequency = 12)
ppi.na = rep(NA, ncol(ppi))
ppi = ts(rbind(ppi, ppi.na), start = c(1990,1), frequency = 12)
# NEW: from 2004
ppi = window(ppi, start = c(2004,1), end = end(ppi))
ppi_jan2004 = ppi[1,]
## Target 3: Nights spent at tourist accommodation (tour) - up to sept - NA for Oct and Nov
# got to getData(2) to download latest tour data
tour <- ts(tour_occ_nim[,-1], start = c(1990,1), frequency = 12)
tour.na = matrix(NA, nrow = 2, ncol = ncol(tour))
tour = ts(rbind(tour, tour.na), start = c(1990,1), frequency = 12)
# NEW: from 2004
tour = window(tour, start = c(2004,1), end = end(tour))
tour_jan2004 = tour[1,c(1,2,4:11,13,14,16:18,20:26)]
tour_may2004 = tour[5,c(12,19)]
tour_jan2005 = tour[13,c(3,15)]
## Remove unneeded stored data
rm(EUc)
rm(hicp.nov)
rm(i)
rm(ppi.na)
rm(words)
rm(tour.na)
rm(tour_occ_nim)
rm(sts_inppd_m)
rm(sts_inpr_m)
rm(pvi.na)
rm(prc_hicp_midx)
rm(nrg_chdd_m)
rm(irt_st_m)
rm(gtrend_data)
rm(gt.country.mat)
rm(gt.country)
rm(google_data)
rm(ei_cphi_m)
#setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Nowcasting/Package/SparseDFM')
setwd('/home/mosleyl/tsclient/mosleyl/OneDrive - Lancaster University/PhD/Nowcasting/Package/SparseDFM/')
library(devtools)
install.packages('devtools')
library(devtools)
