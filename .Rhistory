xfilt
Vfilt <- resul$V
# parÃ¢metros default
model <- rep(1,TT)
model
args
if(args[i] == "model"){ model <- args[[i+1]]
}else if(args[i] == "u"){ u <- args[[i+1]]
}else if(args[i] == "B"){ B <- args[[i+1]]
}
varagin
args = varagin
nargs = length(args)
for(i in seq(1, nargs, by = nargs-1)){
if(args[i] == "model"){ model <- args[[i+1]]
}else if(args[i] == "u"){ u <- args[[i+1]]
}else if(args[i] == "B"){ B <- args[[i+1]]
}
}
model
t=TT-1
m <- model[t+1]
m
t
n = 100 # observations
p = 50 # variables
r = 5 # factors
# simulate factors as a VAR(1)
Sigma_u = 0.5*diag(r)
A = 0.7*diag(r)
factors = VAR_sim(n, A, Sigma_u)
# simulate observations from factors
Sigma_eps = 0.5*diag(p)
Lambda = matrix(rnorm(p*r), ncol = r) # lambda ~ iid N(0,1)
epsilon = MASS::mvrnorm(n, rep(0,p), Sigma_eps)
X = factors %*% t(Lambda) + epsilon
## X_t = L*f_t + e_t, e_t ~ N(0,Sig_e), Sig_e = diag
## f_t = A*f_{t-1} + u_t, u_t ~ N(0,Sig_u)
q=r=5
n = nrow(X)
p = ncol(X)
## STEP 1 - estimate parameters using balanced data
# make balanced
Xbar = na.omit(X)
# standardise for the PCA
Xbar = scale(Xbar)
# PCA - estimate f and L
svd = svd(Xbar)
princomps = (svd$u %*% diag(svd$d))[,1:r]
loadings = svd$v[,1:r]
explained_variance = 1/(nrow(Xbar)-1)*(svd$d^2)[1:r]
# estimate Sig_e
Sig_e = diag(diag(cov(Xbar - princomps %*% t(loadings))))
# estimate VAR(1) A matrix
f_lags = embed(princomps, dimension = 2) # (f_{2:T}, f_{1:T-1})
f_t = f_lags[,1:r]
f_l = f_lags[,(r+1):(2*r)]
A = solve(t(f_l) %*% f_l) %*% t(f_l) %*% f_t
# estimate VAR covariance Sig_u
u <- f_t - f_l %*% A
cov_u <- cov(u)
if(r == q){ # r = q then Sig_u has full rank
Sig_u <- cov_u
}else{ # q < r then Sig_u has rank q
u_svd = svd(u)
u_svd_V = u_svd$v[,1:q]
u_svd_S = diag(1/(nrow(u)-1)*(u_svd$d^2)[1:q])
Sig_u <- u_svd_V %*% u_svd_S %*% t(u_svd_V)
}
factors_pca = princomps
Lam = loadings
## STEP 2 - cast into state space form and use KF & KS to re-estimate factors
## KALMAN FILTER
# f_t|X^{t-1} ~ N(a_t, R_t) # state forecast
# X_t|X^{t-1} ~ N(q_t, Q_t) # innovation
# f_t|X^t ~ N(m_t, C_t) # state update
# f_0 ~ N(m_0, C_0) # initialise
# test ##
A = parametros$A
Lam = parametros$C
Sig_u = parametros$Q
Sig_e = parametros$R
####
m_new = matrix(NA, nrow = n+1, ncol = r)
C_new = array(NA, dim = c(r, r, n+1))
# initialise - m_0 and C_0
m_new[1,] = factors_pca[1,]
C_new[,,1] = matrix(corpcor::pseudoinverse(diag(dim(kronecker(A,A))[1])- kronecker(A,A)) %*% matrix(Sig_u, ncol = 1), r, r)
# test
m_new[1,] = parametros$initx
C_new[,,1] = parametros$initV
for(t in 2:(n+1)) {
# starts at t=1
x = Xbar[t-1,]
# starts at t=0
a_old = m_new[t-1,]
R_old = C_new[,,t-1]
# one step forecast for the state
a_new = A %*% a_old
R_new = A %*% R_old %*% t(A) + Sig_u
# one step ahead forecast for the observation (deal with missing)
q_new = Lam %*% a_new
Sig_e_temp = diag(Sig_e)
Sig_e_temp[is.na(x)] = 1e32
Sig_e = diag(Sig_e_temp)
x[is.na(x)] = 0
Q_new = Lam %*% R_new %*% t(Lam) + Sig_e
# compute posterior at time t (update equations)
innov_error = x - q_new
GG <- t(Lam) %*% diag(1/diag(Sig_e)) %*% Lam # using woodbury identity (see Harvey book)
Q_new_inv <- diag(1/diag(Sig_e)) - diag(1/diag(Sig_e)) %*% Lam %*% corpcor::pseudoinverse(diag(r) + R_new %*% GG) %*% R_new %*% t(Lam) %*% diag(1/diag(Sig_e)) # works only with R diagonal
kalman_gain = R_new %*% t(Lam) %*% Q_new_inv
m_new[t,] = a_new + kalman_gain %*% innov_error
C_new[,,t] = R_new - kalman_gain %*% Lam %*% R_new
# if(log.lik) {
#   d <- dim(innov_error)[1]
#   Q_new_det <- prod(diag(Sig_e)) %*% det(diag(r) + R_new %*% GG)
#   denom <- (2*pi)^(d/2)*sqrt(abs(Q_new_det))
#   mahal <- rowSums(t(innov_error) %*% Q_new_inv %*% innov_error)
#   LL <- -0.5*mahal - log(denom)
#   loglik <- loglik + LL
# }
}
est_filter = m_new[-1,]
cov_filter = C_new[,,-1]
est_filter[1,]
plot(est_filter[,1], type = 'l')
n
ci_upper = ci_lower = c()
for(i in 1:n) {
ci_upper[i] = est_filter[i,1] + 1.96*sqrt(cov_filter[1,1,i])
ci_lower[i] = est_filter[i,1] - 1.96*sqrt(cov_filter[1,1,i])
}
plot(est_filter[,1],type = 'l')
lines(ci_upper, col = 'red')
lines(ci_lower, col='red')
a = matrix(runif(100*100),100,100)
install.packages('microbenchmark')
library(microbenchmark)
microbenchmark(a %*% a)
a = matrix(runif(100*100), 100, 100)
library(microbenchmark)
microbenchmark(a %*% )
microbenchmark(a %*% A)
microbenchmark(a %*% a)
getwd()
# List of useful packages
pkg <- c("tidyr", "dplyr", "ggplot2", "knitr", "rmarkdown")
# Check if packages are not installed and assign the
# names of the uninstalled packages to the variable new.pkg
new.pkg <- pkg[!(pkg %in% installed.packages())]
# If there are any packages in the list that aren't installed,
# install them
if (length(new.pkg)) {
install.packages(new.pkg, repos = "http://cran.rstudio.com")
}
sys.which('pdflatex')
Sys.which('pdflatex')
R.home()
install.packages("tinytex")
library(devtools)
warnings()
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Nowcasting/Package/SparseDFM/')
mydata = exports_DFM_fit
# packages required
library(devtools)
library(roxygen2)
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Nowcasting/Package/SparseDFM/')
# load package
devtools::load_all()
document()
mydata = exports_DFM_fit
View(mydata)
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Services Paper Data Analysis/Data')
exp1 = readRDS('goods_exports_results_1.rds')
exp2 = readRDS('goods_exports_results_2.rds')
exp3 = readRDS('goods_exports_results_3.rds')
exp4 = readRDS('goods_exports_results_4.rds')
# combine fits
#fit.2s = c(exp1$fit.2s, exp2$fit.2s, exp3$fit.2s, exp4$fit.2s)
fit.dfm = c(exp1$fit.dfm, exp2$fit.dfm, exp3$fit.dfm, exp4$fit.dfm)
fit.sdfm = c(exp1$fit.sdfm, exp2$fit.sdfm, exp3$fit.sdfm, exp4$fit.sdfm)
# idx
idx = 49:96
fit.dfm = fit.dfm[[idx]]
length(fit.dfm)
fit.dfm = tail(fit.dfm, 48)
fit.sdfm = tail(fit.sdfm, 48)
fit.dfm[[1]]$data$X[,1]
saveRDS(fit.dfm, 'exports_DFM_fit.rds')
saveRDS(fit.sdfm, 'exports_SDFM_fit.rds')
# data set to upload
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Services Paper Data Analysis/Data')
exports_DFM_fit = readRDS('exports_DFM_fit.rds')
exports_SDFM_fit = readRDS('exports_SDFM_fit.rds')
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Nowcasting/Package/SparseDFM/')
#  add dfm and sdfm results
usethis::use_data(exports_DFM_fit)
#  add dfm and sdfm results
usethis::use_data(exports_DFM_fit, overwrite = TRUE)
usethis::use_data(exports_SDFM_fit, overwrite = TRUE)
# build
devtools::build_rmd("vignettes/exports-example.Rmd")
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Services Paper Data Analysis/Data')
exp1 = readRDS('goods_exports_results_1.rds')
exp2 = readRDS('goods_exports_results_2.rds')
exp3 = readRDS('goods_exports_results_3.rds')
exp4 = readRDS('goods_exports_results_4.rds')
# combine fits
#fit.2s = c(exp1$fit.2s, exp2$fit.2s, exp3$fit.2s, exp4$fit.2s)
fit.dfm = c(exp1$fit.dfm, exp2$fit.dfm, exp3$fit.dfm, exp4$fit.dfm)
fit.sdfm = c(exp1$fit.sdfm, exp2$fit.sdfm, exp3$fit.sdfm, exp4$fit.sdfm)
data_exports = readRDS('exports_dataset.rds')
data_exports = data_exports[-1,]
n = 127
err1.dfm = err2.dfm = c()
for(i in 1:96){
hor1 = fit.dfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = fit.dfm[[i]]$data$fitted.unscaled[n,1:9]
data.obs = data_exports[n-2,1:9]
for1 = data.obs + hor1
for2 = for1 + hor2
err1.dfm[i] = mean(abs(for1 - data_exports[n-1,1:9]))
err2.dfm[i] = mean(abs(for2 - data_exports[n,1:9]))
n = n + 1
}
# sdfm
n = 127
err1.sdfm = err2.sdfm = c()
for(i in 1:96){
hor1 = fit.sdfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = fit.sdfm[[i]]$data$fitted.unscaled[n,1:9]
data.obs = data_exports[n-2,1:9]
for1 = data.obs + hor1
for2 = for1 + hor2
err1.sdfm[i] = mean(abs(for1 - data_exports[n-1,1:9]))
err2.sdfm[i] = mean(abs(for2 - data_exports[n,1:9]))
n = n + 1
}
idx = 49:96
error1.dfm = err1.dfm[idx]
error2.dfm = err2.dfm[idx]
error1.sdfm = err1.sdfm[idx]
error2.sdfm = err2.sdfm[idx]
mean(error1.dfm)
mean(error1.sdfm)
mean(error2.dfm)
mean(error2.sdfm)
quantile(error1.dfm)
quantile(error1.sdfm)
quantile(error2.dfm)
quantile(error2.sdfm)
exports_SDFM_fit[[1]]$data$fitted.unscaled[,1]
fit.sdfm[[49]]$data$fitted.unscaled[,1]
exports_DFM_fit[[1]]$data$fitted.unscaled[,1]
fit.dfm[[49]]$data$fitted.unscaled[,1]
fit.dfm[[96]]$data$fitted.unscaled[,1]
exports_DFM_fit[[48]]$data$fitted.unscaled[,1]
data <- exports
data_exports = data[-1,]
data_exports = data_exports[,1:9]
# n = 127 represents June 2018 (the most recent observed value for the 1st window)
n = 127
dfm <- exports_DFM_fit
sdfm <- exports_SDFM_fit
## Compute errors for the DFM fits at horizon 1 and 2 for each nowcasting window
err1.dfm = err2.dfm = c()
for(i in 1:48){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = dfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = dfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.dfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.dfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
## Compute errors for the Sparse DFM fits at horizon 1 and 2 for each nowcasting window
err1.sdfm = err2.sdfm = c()
for(i in 1:48){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = sdfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = sdfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.sdfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.sdfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
mean(err1.dfm)
fit.dfm = c(exp1$fit.dfm, exp2$fit.dfm, exp3$fit.dfm, exp4$fit.dfm)
fit.sdfm = c(exp1$fit.sdfm, exp2$fit.sdfm, exp3$fit.sdfm, exp4$fit.sdfm)
dfm = fit.dfm
sdfm = fit.sdfm
# n = 127 represents June 2018 (the most recent observed value for the 1st window)
n = 127
## Compute errors for the DFM fits at horizon 1 and 2 for each nowcasting window
err1.dfm = err2.dfm = c()
for(i in 1:48){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = dfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = dfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.dfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.dfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
## Compute errors for the Sparse DFM fits at horizon 1 and 2 for each nowcasting window
err1.sdfm = err2.sdfm = c()
for(i in 1:48){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = sdfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = sdfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.sdfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.sdfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
# n = 127 represents June 2018 (the most recent observed value for the 1st window)
n = 127
## Compute errors for the DFM fits at horizon 1 and 2 for each nowcasting window
err1.dfm = err2.dfm = c()
for(i in 1:96){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = dfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = dfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.dfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.dfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
## Compute errors for the Sparse DFM fits at horizon 1 and 2 for each nowcasting window
err1.sdfm = err2.sdfm = c()
for(i in 1:96){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = sdfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = sdfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.sdfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.sdfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
# n = 127 represents June 2018 (the most recent observed value for the 1st window)
n = 127
err1.dfm = err2.dfm = c()
for(i in 1:96){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = dfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = dfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.dfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.dfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
## Compute errors for the Sparse DFM fits at horizon 1 and 2 for each nowcasting window
err1.sdfm = err2.sdfm = c()
for(i in 1:96){
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = sdfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = sdfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.sdfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.sdfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
# n = 127 represents June 2018 (the most recent observed value for the 1st window)
n = 127
err1.dfm = err2.dfm = c()
for(i in 1:96){
print(i)
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = dfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = dfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.dfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.dfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
err1.sdfm = err2.sdfm = c()
for(i in 1:96){
print(i)
# extract nowcasts for horizon 1 and 2 of the targets
hor1 = sdfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = sdfm[[i]]$data$fitted.unscaled[n,1:9]
# true data
data.obs = data_exports[n-2,]
# nowcasts
for1 = data.obs + hor1
for2 = for1 + hor2
# mean absolute error
err1.sdfm[i] = mean(abs(for1 - data_exports[n-1,]))
err2.sdfm[i] = mean(abs(for2 - data_exports[n,]))
# move window along a month
n = n + 1
}
sdfm[[1]]$data$fitted.unscaled[,1]
length(sdfm[[1]]$data$fitted.unscaled[,1])
n
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Nowcasting/Package/SparseDFM/')
# build
devtools::build_rmd("vignettes/exports-example.Rmd")
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Services Paper Data Analysis/Data')
exp1 = readRDS('goods_exports_results_1.rds')
exp2 = readRDS('goods_exports_results_2.rds')
exp3 = readRDS('goods_exports_results_3.rds')
exp4 = readRDS('goods_exports_results_4.rds')
# combine fits
#fit.2s = c(exp1$fit.2s, exp2$fit.2s, exp3$fit.2s, exp4$fit.2s)
fit.dfm = c(exp1$fit.dfm, exp2$fit.dfm, exp3$fit.dfm, exp4$fit.dfm)
fit.sdfm = c(exp1$fit.sdfm, exp2$fit.sdfm, exp3$fit.sdfm, exp4$fit.sdfm)
# true data - exports
data_exports = readRDS('exports_dataset.rds')
data_exports = data_exports[-1,]
n = 127
err1.dfm = err2.dfm = c()
for(i in 1:96){
hor1 = fit.dfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = fit.dfm[[i]]$data$fitted.unscaled[n,1:9]
data.obs = data_exports[n-2,1:9]
for1 = data.obs + hor1
for2 = for1 + hor2
err1.dfm[i] = mean(abs(for1 - data_exports[n-1,1:9]))
err2.dfm[i] = mean(abs(for2 - data_exports[n,1:9]))
n = n + 1
}
# sdfm
n = 127
err1.sdfm = err2.sdfm = c()
for(i in 1:96){
hor1 = fit.sdfm[[i]]$data$fitted.unscaled[n-1,1:9]
hor2 = fit.sdfm[[i]]$data$fitted.unscaled[n,1:9]
data.obs = data_exports[n-2,1:9]
for1 = data.obs + hor1
for2 = for1 + hor2
err1.sdfm[i] = mean(abs(for1 - data_exports[n-1,1:9]))
err2.sdfm[i] = mean(abs(for2 - data_exports[n,1:9]))
n = n + 1
}
idx = 49:96
error1.dfm = err1.dfm[idx]
error2.dfm = err2.dfm[idx]
error1.sdfm = err1.sdfm[idx]
error2.sdfm = err2.sdfm[idx]
quantile(error1.dfm)
quantile(error1.sdfm)
quantile(error2.dfm)
quantile(error2.sdfm)
mean(error1.dfm)
mean(error1.sdfm)
data_exports[,1]
ts(data_exports[,1], start = c(2004,2), frequency = 12)
ts(data_exports[,1], start = c(2004,2), frequency = 12)[127]
ts(data_exports[,1], start = c(2004,2), frequency = 12)[175]
setwd('C:/Users/mosleyl/OneDrive - Lancaster University/PhD/Nowcasting/Package/SparseDFM')
# build
devtools::build_rmd("vignettes/exports-example.Rmd")
# build
devtools::build_rmd("vignettes/exports-example.Rmd")
devtools::build_rmd("vignettes/inflation-example.Rmd")
library(reshape2)
